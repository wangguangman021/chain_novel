import faiss
import numpy as np
from transformers import AutoTokenizer, AutoModel

def do_add_doc(docs):
    model_name = 'bert-base-uncased'  # 替换为所需的Hugging Face模型的名称

    # 加载tokenizer和model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    embeddings = [tokenizer, model]  # 将tokenizer和model作为embeddings返回
    embed_device = embedding_device()

    # 创建faiss向量索引
    index = faiss.IndexFlatIP(embeddings[1].config.hidden_size) # 使用内积作为相似度度量

    for doc in docs:
        # 将文档内容转化为向量表示
        vector = embeddings[1](embeddings[0](doc.content, return_tensors='pt'))[0]
        
        # 添加文档到向量库
        index.add(vector.numpy())

    # 返回faiss向量索引对象
    return index

def retrieve_doc(query, index, k=5):
    model_name = 'bert-base-uncased'  # 替换为所需的Hugging Face模型的名称

    # 加载tokenizer和model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    # 将查询内容转化为向量表示
    query_vector = model(tokenizer(query, return_tensors='pt'))[0]

    # 使用faiss索引进行检索
    _, doc_indices = index.search(np.array([query_vector.numpy()]), k)

    # 返回最相似的文档列表
    return doc_indices[0]

# 示例用法
docs = [...]  # 文档列表

vector_index_path = '/path/to/vector_index.faiss'
index = do_add_doc(docs)  # 获取向量索引对象

# 保存向量索引到文件
faiss.write_index(index, vector_index_path)

# 加载向量索引
index = faiss.read_index(vector_index_path)

# 进行查询并检索相似的文档
query = "example query"
similar_docs = retrieve_doc(query, index, k=5)
#在上述代码中，并没有明确涉及到调用嵌入（embedding）模型的部分。示例代码中的 compute_vector() 函数是一个自定义函数，用于将文本转化为向量。具体实现细节取决于你使用的嵌入模型。

#如果你的目标是使用嵌入模型将文本转化为向量，你需要根据具体的嵌入模型进行相应的调用。示例代码只包含了存储和查询向量库、以及调用 LLM 模型的部分逻辑，对于嵌入模型的调用部分需要根据你所使用的具体嵌入模型来进行实现。

#记住在 store_document_vectors() 函数中，我们假设已经有了 compute_vector() 函数，它可以将文本转化为向量。你需要根据你的嵌入模型的要求，自定义 compute_vector() 函数的实现。

#因此，在上述代码中，如果你想要调用嵌入模型，则需要根据你使用的嵌入模型，添加相关的代码来计算文本的嵌入向量，并传递给 kb_service 的 add_doc_with_vec() 函数。
