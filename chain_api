import requests
from langchain.docstore import KBServiceFactory

# 创建和加载向量库
kb_service = KBServiceFactory.create()

# 向向量库中存储文档向量
def store_document_vectors(documents):
    for doc in documents:
        vector = compute_vector(doc.text)  # 自定义函数，将文本转化为向量
        kb_service.add_doc_with_vec(doc.id, doc.text, vector)

# 从向量库中查询文档
def search_documents(query):
    results = kb_service.search_docs(query)
    return [doc for doc in results]

# 调用LLM模型的API进行文本生成
def generate_text(prompt):
    url = "http://localhost:8080/llm"  # LLM模型API的地址
    headers = {"Content-Type": "application/json"}
    data = {"prompt": prompt}

    response = requests.post(url, json=data, headers=headers)
    generated_text = response.json()

    return generated_text

# 创建文档对象列表（示例）
documents = [
    Document(id=1, text="This is document 1"),
    Document(id=2, text="Another document here"),
    Document(id=3, text="One more document")
]

# 存储文档向量到向量库
store_document_vectors(documents)

# 查询文档并调用LLM模型进行文本生成
query = "document"
search_results = search_documents(query)

for doc in search_results:
    generated_text = generate_text(doc.text)
    print("Generated text:", generated_text)
#在上述代码中，并没有明确涉及到调用嵌入（embedding）模型的部分。示例代码中的 compute_vector() 函数是一个自定义函数，用于将文本转化为向量。具体实现细节取决于你使用的嵌入模型。

#如果你的目标是使用嵌入模型将文本转化为向量，你需要根据具体的嵌入模型进行相应的调用。示例代码只包含了存储和查询向量库、以及调用 LLM 模型的部分逻辑，对于嵌入模型的调用部分需要根据你所使用的具体嵌入模型来进行实现。

#记住在 store_document_vectors() 函数中，我们假设已经有了 compute_vector() 函数，它可以将文本转化为向量。你需要根据你的嵌入模型的要求，自定义 compute_vector() 函数的实现。

#因此，在上述代码中，如果你想要调用嵌入模型，则需要根据你使用的嵌入模型，添加相关的代码来计算文本的嵌入向量，并传递给 kb_service 的 add_doc_with_vec() 函数。
